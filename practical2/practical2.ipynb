{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical 2: Text Classification\n",
    "<p>Oxford CS - Deep NLP 2017<br>\n",
    "https://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/</p>\n",
    "<p>[Chris Dyer, Yannis Assael, Brendan Shillingford]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"eddb4a6d-1ff7-46fe-ba16-9bf19795f667\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = \"1\";\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force !== \"\") {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      Bokeh.$(\"#eddb4a6d-1ff7-46fe-ba16-9bf19795f667\").text(\"BokehJS successfully loaded.\");\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"eddb4a6d-1ff7-46fe-ba16-9bf19795f667\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'eddb4a6d-1ff7-46fe-ba16-9bf19795f667' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.12.2.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      Bokeh.$(\"#eddb4a6d-1ff7-46fe-ba16-9bf19795f667\").text(\"BokehJS is loading...\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === \"1\")) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === \"1\") {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (!force) {\n",
       "      var cell = $(\"#eddb4a6d-1ff7-46fe-ba16-9bf19795f667\").parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import lxml.etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the dataset if it's not already there: this may take a minute as it is 75MB\n",
    "if not os.path.isfile('ted_en-20160408.zip'):\n",
    "    urllib.request.urlretrieve(\"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\", filename=\"ted_en-20160408.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('ted_en-20160408.zip', 'r') as z:\n",
    "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "\n",
    "root = doc.getroot()\n",
    "\n",
    "del doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels_content = []\n",
    "\n",
    "for file in root:\n",
    "    keywords = file.find(\"head\").find(\"keywords\").text.lower()\n",
    "    content = file.find(\"content\").text.lower()\n",
    "    label = \"\"\n",
    "    label+= (\"T\" if keywords.find(\"technology\")>-1 else \"o\")\n",
    "    label+= (\"E\" if keywords.find(\"entertainment\")>-1 else \"o\")\n",
    "    label+= (\"D\" if keywords.find(\"design\")>-1 else \"o\")\n",
    "    \n",
    "    content = re.sub(r'\\([^)]*\\)', '', content)\n",
    "    \n",
    "    sentences_strings_content = []\n",
    "    for line in content.split('\\n'):\n",
    "        m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
    "        sentences_strings_content.extend(sent for sent in m.groupdict()['postcolon'].split('.') if sent)\n",
    "    \n",
    "    sentences_content = []\n",
    "    for sent_str in sentences_strings_content:\n",
    "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent_str.lower()).split()\n",
    "        sentences_content.append(tokens)\n",
    "        \n",
    "    labels_content.append((label, sentences_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels_content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-af74ebadcd6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# list of training-sentence-token lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraining_sentences_ted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtalk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1585\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtraining_sentences_ted\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mtalk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels_content' is not defined"
     ]
    }
   ],
   "source": [
    "# min occurences of token for inclusion in vocabulary\n",
    "min_count = 5\n",
    "\n",
    "# list of training-sentence-token lists\n",
    "training_sentences_ted = []\n",
    "for talk in labels_content[0:1585]:\n",
    "    training_sentences_ted+=talk[1]\n",
    "\n",
    "# counter and flatten list of tokens\n",
    "training_counts_ted = collections.Counter()\n",
    "training_tokens_ted = [token for sentence in training_sentences_ted for token in sentence]\n",
    "\n",
    "# count all tokens in training-sentence-token lists\n",
    "for token in training_tokens_ted:\n",
    "    training_counts_ted[token] += 1\n",
    "    \n",
    "# replace all tokens with occurence < min_count in list of training-sentence-token lists with \"_____\"\n",
    "for sentence in training_sentences_ted:\n",
    "    for idx in range(0, len(sentence)):\n",
    "        if training_counts_ted[sentence[idx]] < min_count:\n",
    "            sentence[idx] = \"UNKNOWNTEXT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim = 512\n",
    "model_ted = Word2Vec(training_sentences_ted, size=dim, window=5, min_count=min_count, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings_ted = np.empty(shape=(0,dim), dtype=\"float32\")\n",
    "\n",
    "for talk in labels_content:\n",
    "    tokens = [token for sentence in talk[1] for token in sentence]\n",
    "    embedding = np.empty(shape=(1,dim), dtype=\"float32\")\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token not in model_ted.vocab:\n",
    "            token=\"UNKNOWNTEXT\"\n",
    "        embedding+=model_ted[token]\n",
    "        count+=1\n",
    "    embedding/=count\n",
    "    embeddings_ted=np.append(embeddings_ted, embedding,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings = embeddings_ted.astype(\"float32\")\n",
    "labels_strings = np.array([pair[0] for pair in labels_content])\n",
    "\n",
    "# randomize index\n",
    "idxs = np.arange(0, len(embeddings))\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "embeddings = embeddings[idxs]\n",
    "labels_strings = labels_strings[idxs]\n",
    "\n",
    "# correspondence:\n",
    "# ooo - 10000000\n",
    "# Too - 01000000\n",
    "# oEo - 00100000\n",
    "# ooD - 00010000\n",
    "# TEo - 00001000\n",
    "# ToD - 00000100\n",
    "# oED - 00000010\n",
    "# TED - 00000001\n",
    "\n",
    "labels = np.empty(shape=(0,8), dtype=int)\n",
    "\n",
    "for label in labels_strings:\n",
    "    if label == \"ooo\":\n",
    "        labels = np.vstack((labels, np.array([1,0,0,0,0,0,0,0])))\n",
    "    elif label == \"Too\":\n",
    "        labels = np.vstack((labels, np.array([0,1,0,0,0,0,0,0])))\n",
    "    elif label == \"oEo\":\n",
    "        labels = np.vstack((labels, np.array([0,0,1,0,0,0,0,0])))\n",
    "    elif label == \"ooD\":\n",
    "        labels = np.vstack((labels, np.array([0,0,0,1,0,0,0,0])))\n",
    "    elif label == \"TEo\":\n",
    "        labels = np.vstack((labels, np.array([0,0,0,0,1,0,0,0])))\n",
    "    elif label == \"ToD\":\n",
    "        labels = np.vstack((labels, np.array([0,0,0,0,0,1,0,0])))\n",
    "    elif label == \"oED\":\n",
    "        labels = np.vstack((labels, np.array([0,0,0,0,0,0,1,0])))\n",
    "    elif label == \"TED\":\n",
    "        labels = np.vstack((labels, np.array([0,0,0,0,0,0,0,1])))\n",
    "        \n",
    "training_embeddings = embeddings[0:1585]\n",
    "training_labels = labels[0:1585]\n",
    "\n",
    "validation_embeddings = embeddings[1585:1835]\n",
    "validation_labels = labels[1585:1835]\n",
    "\n",
    "testing_embeddings = embeddings[1835:2085]\n",
    "testing_labels = labels[1835:2085]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.05)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one hidden layer\n",
    "x = tf.placeholder(tf.float32, shape=[None, dim])\n",
    "y = tf.placeholder(tf.int32, shape=[None, 8])\n",
    "\n",
    "# fully connected layer\n",
    "W_fc1 = weight_variable([dim, 128])\n",
    "b_fc1 = bias_variable([128])\n",
    "# h_fc1 = tf.tanh(tf.matmul(x, W_fc1) + b_fc1)\n",
    "h_fc1 = tf.nn.relu(tf.matmul(x, W_fc1) + b_fc1)\n",
    "# h_fc1 = tf.sigmoid(tf.matmul(x, W_fc1) + b_fc1)\n",
    "\n",
    "# adding dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# last fully connected layer\n",
    "W_fc2 = weight_variable([128,8])\n",
    "b_fc2 = bias_variable([8])\n",
    "y_pred = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# two hidden layers\n",
    "x = tf.placeholder(tf.float32, shape=[None, dim])\n",
    "y = tf.placeholder(tf.int32, shape=[None, 8])\n",
    "\n",
    "# fully connected layer\n",
    "W_fc1 = weight_variable([dim, 2048])\n",
    "b_fc1 = bias_variable([2048])\n",
    "h_fc1 = tf.tanh(tf.matmul(x, W_fc1) + b_fc1)\n",
    "\n",
    "# second fully connected layer\n",
    "W_fc2 = weight_variable([2048, 1024])\n",
    "b_fc2 = bias_variable([1024])\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "# last fully connected output layer\n",
    "W_fc3 = weight_variable([1024,8])\n",
    "b_fc3 = bias_variable([8])\n",
    "y_pred = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_iterator():\n",
    "    batch_idx = 0\n",
    "    while True:\n",
    "        batch_size = 50\n",
    "        for batch_idx in range(0, len(training_embeddings), batch_size):\n",
    "            embeddings_batch = training_embeddings[batch_idx:batch_idx+batch_size]\n",
    "            labels_batch = training_labels[batch_idx:batch_idx+batch_size]\n",
    "            yield embeddings_batch, labels_batch\n",
    "            \n",
    "iter_ = data_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll use the cross entropy loss function \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_pred))\n",
    "\n",
    "# And classification accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_pred, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# And the Adam optimiser\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=1e-5).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-279-25ffbcbda69c>:2: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 499, training accuracy 0.5\n",
      "step 999, training accuracy 0.56\n",
      "step 1499, training accuracy 0.5\n",
      "step 1999, training accuracy 0.62\n",
      "step 2499, training accuracy 0.46\n",
      "step 2999, training accuracy 0.58\n",
      "step 3499, training accuracy 0.66\n",
      "step 3999, training accuracy 0.571429\n",
      "step 4499, training accuracy 0.5\n",
      "step 4999, training accuracy 0.56\n",
      "0.556\n"
     ]
    }
   ],
   "source": [
    "for i in range(5000):\n",
    "    embeddings_batch, labels_batch = iter_.__next__()\n",
    "    if (i+1)%500==0:\n",
    "        train_accuracy = accuracy.eval(session = sess, feed_dict={ x:embeddings_batch, y: labels_batch, keep_prob: 1})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(session = sess, feed_dict={x: embeddings_batch, y: labels_batch, keep_prob: .2})\n",
    "    \n",
    "print(accuracy.eval(session = sess, feed_dict={x: validation_embeddings, y: validation_labels, keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.556\n",
      "18168\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for par in labels_strings[1585:1835]:\n",
    "    if par==\"ooo\":\n",
    "        count+=1\n",
    "print(count / 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do the tSNE thing to get it down to two dimensions"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
